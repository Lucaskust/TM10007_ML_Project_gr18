{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Vereiste imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn preprocessing, feature selection, and decomposition\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, RFECV, SequentialFeatureSelector, f_classif\n",
    "\n",
    "# Scikit-learn models and evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Additional imports\n",
    "import pickle\n",
    "\n",
    "# üßæ Stap 1 ‚Äì Laad de data\n",
    "data = pd.read_excel(\"..\\..\\TrainData.xlsx\")\n",
    "\n",
    "# üîç Eerste inspectie\n",
    "print(\"Vorm van de data:\", data.shape)\n",
    "print(\"Kolommen:\", data.columns.tolist())\n",
    "print(\"Aantal duplicaten:\", data.duplicated().sum())\n",
    "print(\"Missende waarden per kolom:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# üßº Stap 2 ‚Äì Dubbele rijen verwijderen\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# üéØ Stap 3 ‚Äì Split in features en labels\n",
    "X = data.drop(columns=\"label\")\n",
    "y = data[\"label\"]\n",
    "\n",
    "# üîÅ Zet y om naar numeriek met behoud van pandas Series\n",
    "label_encoder = LabelEncoder()\n",
    "y = pd.Series(label_encoder.fit_transform(y), index=y.index)\n",
    "\n",
    "# ‚ùì Hoeveel missende waarden blijven over?\n",
    "print(\"Totaal aantal missende waarden:\", X.isnull().sum().sum())\n",
    "\n",
    "# ‚ö†Ô∏è Stap 4 ‚Äì NaNs imputer (mediaan)\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "\n",
    "#Data normalisatie\n",
    "\n",
    "X_Scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra function to include RandomForest feature importance in KNN\n",
    "\n",
    "class KNNWithFeatureImportance(KNeighborsClassifier):\n",
    "    def __init__(self, n_neighbors=5, **kwargs):\n",
    "        super().__init__(n_neighbors=n_neighbors, **kwargs)\n",
    "        self.feature_importances_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Call the original fit method\n",
    "        super().fit(X, y)\n",
    "        \n",
    "        # Create a simple feature importance based on feature variance\n",
    "        # This is just one possible approach - you could use other methods\n",
    "        rf_model = RandomForestClassifier(n_estimators=50)\n",
    "        rf_model.fit(X, y)\n",
    "        \n",
    "        self.feature_importances_ = rf_model.feature_importances_\n",
    "        \n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code defines two pipelines for hyperparameter tuning using GridSearchCV.\n",
    "# Both pipelines aim to optimize the K-Nearest Neighbors (KNN) classifier with different feature selection methods.\n",
    "\n",
    "# Pipeline 1:\n",
    "# - Uses SelectKBest for univariate feature selection.\n",
    "# - Applies Recursive Feature Elimination with Cross-Validation (RFECV) to further refine feature selection.\n",
    "# - Reduces dimensionality using PCA.\n",
    "# - Trains the KNN classifier with a custom feature importance implementation (KNNWithFeatureImportance).\n",
    "# The hyperparameters for SelectKBest, PCA, and KNN are tuned using GridSearchCV.\n",
    "\n",
    "# Pipeline 2:\n",
    "# - Similar to Pipeline 1 but replaces RFECV with Sequential Feature Selector for feature selection.\n",
    "# - Trains a standard KNN classifier.\n",
    "# The hyperparameters for SelectKBest, PCA, and KNN are also tuned using GridSearchCV.\n",
    "\n",
    "# Both pipelines save their respective GridSearchCV results\n",
    "\n",
    "\n",
    "knnIMPORTANCE = KNNWithFeatureImportance()\n",
    "\n",
    "# Define the pipeline 1\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection_1', SelectKBest()), \n",
    "    ('feature_selection_2', RFECV(knnIMPORTANCE, step=1, cv=StratifiedKFold(5), scoring='roc_auc')),\n",
    "    ('pca', PCA()),  # PCA to reduce dimensionality\n",
    "    ('knn', knnIMPORTANCE)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"feature_selection_1__k\": [80, 90, 100, 110, 120],\n",
    "    \"knn__n_neighbors\": [5, 7, 9, 11, 15, 20],\n",
    "    \"pca__n_components\": [10, 12, 14, 14+2, 18, 20, 0.9999]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=StratifiedKFold(5), scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_Scaled, y)\n",
    "\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "results_df.to_excel(\"Results_grid_search\\grid_search_KNN_results_RFECV.xlsx\", index=False)\n",
    "\n",
    "#PIPELINE 2\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "pipeline_2 = Pipeline([\n",
    "    ('feature_selection_1', SelectKBest()), \n",
    "    ('feature_selection_2',SequentialFeatureSelector(KNN, direction='forward', scoring='roc_auc', n_jobs=-1)),\n",
    "    ('pca', PCA()),  # PCA to reduce dimensionality\n",
    "    ('knn', KNN)\n",
    "])\n",
    "\n",
    "param_grid_2 = {\n",
    "    \"feature_selection_1__k\": [80, 90, 100, 110, 120],\n",
    "    \"knn__n_neighbors\": [5, 7, 9, 11, 15, 20],\n",
    "    \"pca__n_components\": [10, 12, 14, 14+2, 18, 20, 0.9999]\n",
    "}\n",
    "\n",
    "grid_search_2 = GridSearchCV(pipeline_2, param_grid_2, cv=StratifiedKFold(5), scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "grid_search_2.fit(X_Scaled, y)\n",
    "\n",
    "results_df = pd.DataFrame(grid_search_2.cv_results_)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "results_df.to_excel(\"Results_grid_search\\grid_search_KNN_results_forward.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code performs a final grid search to fine-tune the hyperparameters of the KNN classifier.\n",
    "# It uses a pipeline with the following steps:\n",
    "# - SelectKBest: Selects the top 80 features based on univariate statistical tests.\n",
    "# - RFECV: Applies Recursive Feature Elimination with Cross-Validation to further refine feature selection.\n",
    "# - PCA: Reduces the dimensionality of the data to 20 components.\n",
    "# - KNNWithFeatureImportance: A custom KNN classifier that incorporates feature importance.\n",
    "\n",
    "# The grid search optimizes the hyperparameters for SelectKBest, PCA, and KNN:\n",
    "# - \"feature_selection_1__k\": Number of features to select in SelectKBest.\n",
    "# - \"knn__n_neighbors\": Number of neighbors for the KNN classifier.\n",
    "# - \"pca__n_components\": Number of principal components to retain in PCA.\n",
    "\n",
    "# The results of the grid search are saved to an Excel file for further analysis.\n",
    "\n",
    "knnIMPORTANCE = KNNWithFeatureImportance()\n",
    "\n",
    "# Define the pipeline 1\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selection_1', SelectKBest()), \n",
    "    ('feature_selection_2', RFECV(knnIMPORTANCE, step=1, cv=StratifiedKFold(5), scoring='roc_auc')),\n",
    "    ('pca', PCA()),  # PCA to reduce dimensionality\n",
    "    ('knn', knnIMPORTANCE)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"feature_selection_1__k\": [50, 55, 60, 65, 70, 75, 80, 85],\n",
    "    \"knn__n_neighbors\": [3, 5],\n",
    "    \"pca__n_components\": [10, 12, 14, 16, 18, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=StratifiedKFold(5), scoring='roc_auc', n_jobs=-2)\n",
    "\n",
    "grid_search.fit(X_Scaled, y)\n",
    "\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "results_df.to_excel(\"grid_search_KNN_results_RECV_2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines a pipeline for further hyperparameter tuning of a KNN classifier with SequentialFeatureSelector:\n",
    "# - Step 1: SelectKBest selects the top 80 features based on univariate statistical tests.\n",
    "# - Step 2: SequentialFeatureSelector performs forward feature selection to refine the feature set.\n",
    "# - Step 3: PCA reduces the dimensionality of the data.\n",
    "# - Step 4: KNN classifier is trained on the processed features.\n",
    "# A GridSearchCV is used to optimize the hyperparameters for SelectKBest, SequentialFeatureSelector, PCA, and KNN.\n",
    "# The results of the grid search are saved to an Excel file for further analysis.\n",
    "\n",
    "\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "clf = SelectKBest(k=80)\n",
    "clf.fit(X_Scaled, y)\n",
    "X_k_best = clf.transform(X_Scaled)\n",
    "selected_features = clf.get_support(indices=True)\n",
    "\n",
    "\n",
    "pipeline_4 = Pipeline([\n",
    "  #  ('feature_selection_1', SelectKBest()), \n",
    "    ('feature_selection_2',SequentialFeatureSelector(KNN, direction='forward',cv=StratifiedKFold(5), scoring='roc_auc', n_jobs=-1)),\n",
    "  #  ('pca', PCA()),  # PCA to reduce dimensionality\n",
    "    ('knn', KNN)\n",
    "])\n",
    "\n",
    "param_grid_4 = {\n",
    "    #\"feature_selection_1__k\": [80],\n",
    "    'feature_selection_2__n_features_to_select': [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    \"knn__n_neighbors\": [5, 7, 9, 11, 15, 20],\n",
    "   # \"pca__n_components\": [0.9999]\n",
    "}\n",
    "\n",
    "grid_search_4 = GridSearchCV(pipeline_4, param_grid_4, cv=StratifiedKFold(5), scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "grid_search_4.fit(X_k_best, y)\n",
    "\n",
    "results_df = pd.DataFrame(grid_search_4.cv_results_)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "results_df.to_excel(r\"Results_grid_search\\grid_search_KNN_results_forward_2.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jelle\\OneDrive - Delft University of Technology\\10007 Machine learning\\.conda\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [ 76  77  83  84  85  90  96  97 103 109 110 467 474 475 480 487 488] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\jelle\\OneDrive - Delft University of Technology\\10007 Machine learning\\.conda\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "# Save the final trained KNN model along with feature masks and scaler\n",
    "\n",
    "# Step 1: Standardize the features using StandardScaler\n",
    "scaler_standard = StandardScaler()\n",
    "X_Scaled = scaler_standard.fit_transform(X)\n",
    "\n",
    "# Step 2: Perform feature selection using SelectKBest\n",
    "selector_KBest = SelectKBest(k=80)  # Select the top 80 features\n",
    "X_kbest = selector_KBest.fit_transform(X_Scaled, y)\n",
    "X_kbest_features_mask = selector_KBest.get_support()  # Boolean mask of selected features\n",
    "\n",
    "# Step 3: Initialize the custom KNN classifier with feature importance\n",
    "#KNN = KNNWithFeatureImportance(n_neighbors=5)\n",
    "KNN = KNeighborsClassifier(n_neighbors=9)\n",
    "\n",
    "# Step 4: Perform Recursive Feature Elimination with Cross-Validation (RFECV)\n",
    "#selector_RFECV = RFECV(KNN, step=1, cv=StratifiedKFold(5), scoring='roc_auc')\n",
    "#X_selected = selector_RFECV.fit_transform(X_kbest, y)\n",
    "#X_RFECV_features_mask = selector_RFECV.get_support()  # Boolean mask of selected features after RFECV\n",
    "selector_forward = SequentialFeatureSelector(KNN, n_features_to_select= 9, direction='forward', cv=StratifiedKFold(5), scoring='roc_auc', n_jobs=-1)\n",
    "X_selected = selector_forward.fit_transform(X_kbest, y)\n",
    "X_forward_features_mask = selector_forward.get_support()  # Boolean mask of selected features after RFECV\n",
    "\n",
    "# Step 5: Combine the feature masks from SelectKBest and selector\n",
    "X_features_mask_total = np.zeros(len(X_kbest_features_mask), dtype=bool)  # Initialize a mask with all False\n",
    "indices_after_X_kbest_features_mask = np.where(X_kbest_features_mask)[0]  # Indices of features selected by SelectKBest\n",
    "indices_to_keep = indices_after_X_kbest_features_mask[X_forward_features_mask]  # Apply  mask to SelectKBest features\n",
    "X_features_mask_total[indices_to_keep] = True  # Update the combined mask\n",
    "\n",
    "# Step 6: Train the final KNN model on the selected features\n",
    "KNN.fit(X_selected, y)\n",
    "\n",
    "# Step 7: Save the trained KNN model, combined feature mask, and scaler to disk\n",
    "with open(r'KNNFinal\\KNN_model.pkl', 'wb') as file:\n",
    "    pickle.dump(KNN, file)  # Save the trained KNN model\n",
    "\n",
    "with open(r'KNNFinal\\selected_features_mask.pkl', 'wb') as file:\n",
    "    pickle.dump(X_features_mask_total, file)  # Save the combined feature mask\n",
    "\n",
    "with open(r'KNNFinal\\KNN_Scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_standard, file)  # Save the scaler used for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code was used to try different things and plot the graphs\n",
    "\n",
    "#k-best\n",
    "clf = SelectKBest(k=120)\n",
    "clf.fit(X_Scaled, y)\n",
    "X_k_best = clf.transform(X_Scaled)\n",
    "selected_features = clf.get_support(indices=True)\n",
    "\n",
    "#RFECV met KNN\n",
    "clf = KNNWithFeatureImportance(n_neighbors=5)\n",
    "selector = RFECV(clf, step=1, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "X_selected = selector.fit_transform(X_k_best, y)\n",
    "\n",
    "\n",
    "# make a plot of the KNNWithFeatureImportance\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross-validation score (ROC AUC)\")\n",
    "plt.title(\"Recursive Feature Elimination with Cross-Validation (KNN=5)\")\n",
    "plt.grid(True)\n",
    "plt.plot(range(1, len(selector.cv_results_['mean_test_score']) + 1), selector.cv_results_['mean_test_score'])\n",
    "plt.show()\n",
    "\n",
    "#SequentialFeatureSelector\n",
    "clf = KNeighborsClassifier(n_neighbors=9)\n",
    "selector = SequentialFeatureSelector(clf, direction='forward', cv=StratifiedKFold(5))\n",
    "X_selected = selector.fit_transform(X_k_best, y)\n",
    "\n",
    "# make a plot of the KNNWithFeatureImportance\n",
    "cv = StratifiedKFold(5)\n",
    "scores = []\n",
    "for n_features in range(1, X_k_best.shape[1] + 1):\n",
    "    selector = SequentialFeatureSelector(clf, n_features_to_select=n_features, direction='forward', cv=cv)\n",
    "    X_selected = selector.fit_transform(X_k_best, y)\n",
    "    score = np.mean(cross_val_score(clf, X_selected, y, cv=cv, scoring='roc_auc'))\n",
    "    scores.append(score)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross-validation score (ROC AUC)\")\n",
    "plt.title(\"Forward Feature Elimination with Cross-Validation (KNN=9)\")\n",
    "plt.grid(True)\n",
    "plt.plot(range(1, len(scores) + 1), scores)\n",
    "plt.show()\n",
    "\n",
    "#general testcode\n",
    "KNN = KNeighborsClassifier(n_neighbors=4+1)\n",
    "scores = cross_validate(KNN, X_selected, y, cv=StratifiedKFold(10), scoring=[\"roc_auc\", \"accuracy\"])\n",
    "print(f\"AUC = {scores['test_roc_auc'].mean()} and accuracy = {scores['test_accuracy'].mean()}\")\n",
    "\n",
    "\n",
    "# using PCA to reduce the dimensionality of the data\n",
    "from sklearn.decomposition import PCA\n",
    "for i in [22]:\n",
    "    pca = PCA(n_components=i) \n",
    "    X_pca = pca.fit_transform(X_selected)\n",
    "    KNN = KNeighborsClassifier(n_neighbors=4+1)\n",
    "    scores = cross_validate(KNN, X_pca, y, cv=StratifiedKFold(10), scoring=[\"roc_auc\", \"accuracy\"])\n",
    "    print(f\"AUC = {scores['test_roc_auc'].mean()} and accuracy = {scores['test_accuracy'].mean()}\")\n",
    "\n",
    "\n",
    "# baseline test\n",
    "scores = cross_validate(KNN, X, y, cv=StratifiedKFold(10), scoring=[\"roc_auc\", \"accuracy\"])\n",
    "print(f\"AUC = {scores['test_roc_auc'].mean()} and accuracy = {scores['test_accuracy'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_k_best.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jelle\\OneDrive - Delft University of Technology\\10007 Machine learning\\.conda\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [ 76  77  83  84  85  90  96  97 103 109 110 467 474 475 480 487 488] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\jelle\\OneDrive - Delft University of Technology\\10007 Machine learning\\.conda\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "#k-best\n",
    "clf = SelectKBest(k=80)\n",
    "clf.fit(X_Scaled, y)\n",
    "X_k_best = clf.transform(X_Scaled)\n",
    "selected_features = clf.get_support(indices=True)\n",
    "\n",
    "\n",
    "KNN = KNeighborsClassifier(n_neighbors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8478806907378335)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(5)\n",
    "\n",
    "selector = SequentialFeatureSelector(KNN, n_features_to_select=9, direction='forward', cv=cv, n_jobs=-1)\n",
    "X_selected = selector.fit_transform(X_k_best, y)\n",
    "np.mean(cross_val_score(KNN, X_selected, y, cv=cv, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8552197802197803)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=0.99) \n",
    "X_pca = pca.fit_transform(X_selected)\n",
    "\n",
    "\n",
    "np.mean(cross_val_score(KNN, X_pca, y, cv=cv, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of the SequentialFeatureSelector\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "scores = []\n",
    "for n_features in range(1, X_k_best.shape[1] + 1):\n",
    "    selector = SequentialFeatureSelector(KNN, n_features_to_select=n_features, direction='forward', cv=cv, n_jobs=-1)\n",
    "    X_selected = selector.fit_transform(X_k_best, y)\n",
    "    score = np.mean(cross_val_score(KNN, X_selected, y, cv=cv, scoring='roc_auc'))\n",
    "    scores.append(score)\n",
    "    print(score)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross-validation score (ROC AUC)\")\n",
    "plt.title(\"Forward Feature Elimination with Cross-Validation (KNN=9)\")\n",
    "plt.grid(True)\n",
    "plt.plot(range(1, len(scores) + 1), scores)\n",
    "plt.savefig(r\"pictures\\KNN_SequentialFeatureSelector.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
