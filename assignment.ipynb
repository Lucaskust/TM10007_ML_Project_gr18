{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDn2Sk-VWqE",
        "outputId": "9f162265-cd6c-4b68-fc17-2b6c51b2e291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m852.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ktml (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VfiXrczIa0s"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "92e77de9-3bca-4f23-e8ce-efe538c394a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of samples: 186\n",
            "The number of columns: 494\n"
          ]
        }
      ],
      "source": [
        "# Data loading functions. Uncomment the one you want to use\n",
        "#from worcgist.load_data import load_data\n",
        "#from worclipo.load_data import load_data\n",
        "from worcliver.load_data import load_data\n",
        "#from ecg.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-processing\n",
        "Feature Cleaning:\n",
        "\n",
        "1. Loading the full dataset from Excel.\n",
        "2. Identifying and removing features with excessive outliers using the IQR method.\n",
        "3. Winsorizing (capping) extreme values in the remaining features to reduce their influence.\n",
        "4. Printing the new dataset dimensions and how many features were removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "JZ-2aM8UJx_C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (186, 494)\n",
            "Shape after cleaning: (186, 487)\n",
            "Removed 7 features with more than 17 outliers.\n",
            "Shape before removing constant features: (186, 486)\n",
            "Shape after removing constant features: (186, 452)\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from pathlib import Path\n",
        "\n",
        "current_dir = Path.cwd()\n",
        "dir_results = current_dir / 'Results'\n",
        "\n",
        "# Load the dataset from an Excel file into a DataFrame\n",
        "df = pd.read_excel('FullDataset.xlsx')\n",
        "print(f\"Original shape: {df.shape}\")  # Shows initial number of rows and columns (samples, features)\n",
        "\n",
        "# Keep only the numeric columns (e.g., float, int) for outlier analysis\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# Dictionary to store number of outliers found in each column\n",
        "outlier_counts = {}\n",
        "\n",
        "# Loop over all numeric features to detect outliers\n",
        "for col in numeric_df.columns:\n",
        "    Q1 = numeric_df[col].quantile(0.25)     # First quartile (25th percentile)\n",
        "    Q3 = numeric_df[col].quantile(0.75)     # Third quartile (75th percentile)\n",
        "    IQR = Q3 - Q1                           # Interquartile range\n",
        "    lower_bound = Q1 - 5 * IQR              # Conservative lower outlier threshold\n",
        "    upper_bound = Q3 + 5 * IQR              # Conservative upper outlier threshold\n",
        "\n",
        "    # Identify outliers and count them\n",
        "    outliers = numeric_df[(numeric_df[col] < lower_bound) | (numeric_df[col] > upper_bound)]\n",
        "    outlier_counts[col] = len(outliers)\n",
        "\n",
        "# Create a DataFrame from the dictionary to make it easy to sort/view\n",
        "outlier_df = pd.DataFrame.from_dict(outlier_counts, orient='index', columns=['outlier_count'])\n",
        "outlier_df = outlier_df.sort_values(by='outlier_count', ascending=False)\n",
        "\n",
        "# Define a threshold: if a feature has more than 17 outliers, we consider it too noisy\n",
        "threshold = 17\n",
        "features_to_drop = outlier_df[outlier_df['outlier_count'] > threshold].index.tolist()\n",
        "\n",
        "# Drop those features from the original DataFrame\n",
        "df = df.drop(columns=features_to_drop)\n",
        "\n",
        "# Identify the features that are still allowed (with 17 or fewer outliers)\n",
        "features_to_winsorize = outlier_df[outlier_df['outlier_count'] <= threshold].index.tolist()\n",
        "\n",
        "# Apply outlier capping to these \"acceptable\" features\n",
        "for feature in features_to_winsorize:\n",
        "    if feature in df.columns:\n",
        "        Q1 = df[feature].quantile(0.25)\n",
        "        Q3 = df[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 5 * IQR\n",
        "        upper_bound = Q3 + 5 * IQR\n",
        "\n",
        "        # Clip values that fall outside the acceptable range\n",
        "        df[feature] = df[feature].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "# Print new shape of the cleaned dataset\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Shape after cleaning: {df.shape}\")\n",
        "print(f\"Removed {len(features_to_drop)} features with more than {threshold} outliers.\")\n",
        "\n",
        "numeric_df = df.select_dtypes(include='number').drop(columns=['label'])\n",
        "X_clean = numeric_df.values\n",
        "\n",
        "print(f\"Shape before removing constant features: {X_clean.shape}\")\n",
        "\n",
        "# Verwijder constant features (met variantie == 0)\n",
        "selector = VarianceThreshold(threshold=0.0)\n",
        "X_clean = selector.fit_transform(X_clean)\n",
        "y_clean = df['label'].values\n",
        "\n",
        "print(f\"Shape after removing constant features: {X_clean.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function performs nested cross-validation for robust model evaluation and hyperparameter tuning using:\n",
        "- Outer loop: Evaluates generalization performance\n",
        "- Inner loop: Tunes hyperparameters using GridSearchCV\n",
        "- RFECV: Performs recursive feature elimination within each outer fold\n",
        "\n",
        "It returns performance scores, selected hyperparameters, and feature selection masks across multiple repeated trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "# difficult nested score to see best params. Perform grid-search with nested cross-validation for hyperparameter tuning\n",
        "def safe_f_classif(X, y):\n",
        "    \"\"\"Wrapper around sklearn's _f_classif to handle divide-by-zero.\"\"\"\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        return f_classif(X, y)\n",
        "    \n",
        "def nested_cross_validation(NUM_TRIALS : int, clf_class, param_grid : dict, X : np.ndarray, y : np.ndarray, mask = None) -> tuple:\n",
        "    \"\"\"\n",
        "    Perform nested cross-validation for hyperparameter tuning.\n",
        "    This function uses RFECV for feature selection and SelectKBest for feature selection.\n",
        "    It evaluates the model using accuracy as the scoring metric.\n",
        "    \n",
        "    Parameters:\n",
        "    - NUM_TRIALS: Number of trials to run\n",
        "    - clf_class: The machine learning classifier to evaluate\n",
        "    - param_grid: Hyperparameter grid for GridSearchCV\n",
        "    - X: Feature matrix\n",
        "    - y: Target vector\n",
        "    \n",
        "    Returns:\n",
        "    - nested_scores: Array of scores from each trial\n",
        "    - df_best_params: DataFrame of best parameters for each trial\n",
        "    - df_masks: DataFrame of all feature selection masks\n",
        "    \"\"\"\n",
        "    \n",
        "    nested_scores = np.zeros(NUM_TRIALS)      # To store average scores per trial\n",
        "    all_best_params = []                      # To store best hyperparameters per trial\n",
        "    all_feature_masks = []                    # To store feature selection masks per trial\n",
        "\n",
        "    for i in range(NUM_TRIALS):\n",
        "        # Outer and inner stratified cross-validation splitters\n",
        "        outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
        "        RFECV_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i+1000)\n",
        "        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i+2000)\n",
        "\n",
        "        # Store the best parameters and scores for each outer fold\n",
        "        best_params = []     # For storing best parameters for each outer fold\n",
        "        outer_scores = []    # AUC scores per outer fold\n",
        "        trial_masks = []     # Feature selection masks per outer fold\n",
        "\n",
        "        k = 0\n",
        "        for train_idx, test_idx in outer_cv.split(X, y):\n",
        "            k += 1\n",
        "            print(f\"Outer fold {k}\")\n",
        "            # Split data into training and test for this outer fold\n",
        "            X_train, X_test = X[train_idx], X[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "            \n",
        "            # Feature selection via SelectKBest using the outer CV folds\n",
        "            # SelectKBest is used to select the top k features based on ANOVA F-value\n",
        "            # You can delete this if you want to use all features, computation time will be longer!\n",
        "            # selector_kbest = SelectKBest(score_func=safe_f_classif, k=100)              \n",
        "            \n",
        "            # Fit the selector on the training data, not the entire dataset\n",
        "            # This can be deteled, but it will take longer to compute!\n",
        "            # Fit SelectKBest on outer training set\n",
        "            # and transform both the training and test sets\n",
        "            # selector_kbest.fit(X_train, y_train)  # Fit SelectKBest on outer training set\n",
        "            # X_train = selector_kbest.transform(X_train)  # Transform outer training set \n",
        "            # X_test = selector_kbest.transform(X_test)  # Transform outer test set        \n",
        "            \n",
        "            if mask is None:\n",
        "            # Feature selection via RFECV using the inner CV folds\n",
        "                selector_RFECV = RFECV(\n",
        "                    estimator=RandomForestClassifier(),\n",
        "                    step=1,\n",
        "                    cv=RFECV_cv,\n",
        "                    scoring='accuracy'\n",
        "                    )\n",
        "\n",
        "\n",
        "                selector_RFECV.fit(X_train, y_train)  # Fit RFECV on outer training set  \n",
        "                trial_masks.append(selector_RFECV.support_)  # Save which features were kept\n",
        "                print(\"RFECV done\")\n",
        "                # Reduce training and test sets to selected features\n",
        "                X_train = selector_RFECV.transform(X_train)\n",
        "                X_test = selector_RFECV.transform(X_test)\n",
        "            else:\n",
        "                # Use the provided mask to select features\n",
        "                \n",
        "                mask_used = mask[f'outer_{k}'].values[i-1]\n",
        "\n",
        "                X_train = X_train[:, mask_used]\n",
        "                X_test = X_test[:, mask_used]\n",
        "                trial_masks.append(mask_used)\n",
        "            # Build a fresh pipeline for each fold\n",
        "            pipeline = Pipeline([\n",
        "                (\"scaler\", MinMaxScaler()),       # Normalize features to [0, 1]\n",
        "                (\"clf_class\", clf_class)          # Insert model\n",
        "            ])\n",
        "                        \n",
        "            # Create a new GridSearchCV for each inner fold\n",
        "            grid_search = GridSearchCV(pipeline, param_grid, cv=inner_cv, scoring=\"accuracy\", n_jobs=-1)\n",
        "            \n",
        "            # Fit GridSearchCV on the training data\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            \n",
        "            # Get the best model from inner CV\n",
        "            best_model = grid_search.best_estimator_\n",
        "            \n",
        "            # Evaluate the best model on the outer test fold\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            outer_score = accuracy_score(y_test, y_pred)\n",
        "            # Append the best parameters and outer score\n",
        "            best_params.append(grid_search.best_params_)\n",
        "            outer_scores.append(outer_score)\n",
        "        \n",
        "        # Store the mean score for this trial\n",
        "        nested_scores[i] = np.mean(outer_scores)\n",
        "        all_best_params.append([i, best_params])\n",
        "        all_feature_masks.append([i, trial_masks])\n",
        "\n",
        "    # Format best params and feature masks into DataFrames\n",
        "    df_best_params = pd.DataFrame([best_params for _, best_params in all_best_params], columns=[\"outer_1\", \"outer_2\", \"outer_3\", \"outer_4\", \"outer_5\"])\n",
        "    df_masks = pd.DataFrame([trial_masks for _, trial_masks in all_feature_masks], columns=[\"outer_1\", \"outer_2\", \"outer_3\", \"outer_4\", \"outer_5\"])\n",
        "\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Average performance across {NUM_TRIALS} trials: {np.mean(nested_scores):.4f} ± {np.std(nested_scores):.4f}\")\n",
        "\n",
        "    return nested_scores, df_best_params, df_masks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code for the KNN classifier\n",
        "This code is used to perform nested cross-validation for hyperparameter tuning of a K-Nearest Neighbors (KNN) classifier.\n",
        "It includes feature selection using SelectKBest and RFECV, and it evaluates the model's performance using accuracy as the scoring metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outer fold 1\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'clf_class__n_neighbors': [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25],\n",
        "}\n",
        "\n",
        "clf_class = KNeighborsClassifier()\n",
        "\n",
        "# NUM_TRIALS = 1 -> could be changed when stronger computer available\n",
        "nested_scores_knn, df_best_params_knn, df_masks_knn = nested_cross_validation(1,clf_class, param_grid, X_clean, y_clean)\n",
        "\n",
        "# Save the results to CSV files\n",
        "df_best_params_knn.to_csv(dir_results / \"best_params_knn.csv\", index=False)\n",
        "df_masks_knn.to_csv(dir_results / 'masks_knn.csv', index=False)\n",
        "\n",
        "for i in df_masks_knn:\n",
        "    print(f\"{i}: {df_masks_knn[i].values[0].sum()} features selected\")\n",
        "\n",
        "best_params_knn_ravel= pd.Series(df_best_params_knn.values.ravel())\n",
        "best_params_knn_ravel.value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code for the Naive Bayes\n",
        "This code is used to perform nested cross-validation for hyperparameter tuning of a Naive Bayes classifier.\n",
        "It includes feature selection using SelectKBest and RFECV, and it evaluates the model's performance using accuracy as the scoring metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outer fold 1\n",
            "Outer fold 2\n",
            "Outer fold 3\n",
            "Outer fold 4\n",
            "Outer fold 5\n",
            "Average performance across 1 trials: 0.7100 ± 0.0000\n",
            "outer_1: 57 features selected\n",
            "outer_2: 72 features selected\n",
            "outer_3: 61 features selected\n",
            "outer_4: 69 features selected\n",
            "outer_5: 64 features selected\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'clf_class__var_smoothing': 0.1}      3\n",
              "{'clf_class__var_smoothing': 1e-12}    2\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "param_grid = {\n",
        "    'clf_class__var_smoothing': [1*10**-12, 1*10**-11, 1*10**-10, 1*10**-9, 1*10**-8, 1*10**-7, 1*10**-6, 1*10**-5, 1*10**-4, 1*10**-3, 1*10**-2, 1*10**-1],\n",
        "}\n",
        "\n",
        "clf_class = GaussianNB()\n",
        "\n",
        "# NUM_TRIALS = 1 -> could be changed when stronger computer available\n",
        "nested_scores_naive, df_best_params_naive, df_masks_naive = nested_cross_validation(1,clf_class, param_grid, X_clean, y_clean, df_masks_knn)\n",
        "\n",
        "# Save the results to CSV files\n",
        "df_best_params_naive.to_csv(dir_results / \"best_params_naive.csv\", index=False)\n",
        "df_masks_naive.to_csv(dir_results / 'masks_naive.csv', index=False)\n",
        "\n",
        "for i in df_masks_naive:\n",
        "    print(f\"{i}: {df_masks_naive[i].values[0].sum()} features selected\")\n",
        "\n",
        "best_params_naive_ravel= pd.Series(df_best_params_naive.values.ravel())\n",
        "best_params_naive_ravel.value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code for the Random Forest Classifier\n",
        "This code is used to perform nested cross-validation for hyperparameter tuning of a Random Forest classifier.\n",
        "It includes feature selection using SelectKBest and RFECV, and it evaluates the model's performance using accuracy as the scoring metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outer fold 1\n",
            "Outer fold 2\n",
            "Outer fold 3\n",
            "Outer fold 4\n",
            "Outer fold 5\n",
            "Average performance across 1 trials: 0.7149 ± 0.0000\n",
            "outer_1: 57 features selected\n",
            "outer_2: 72 features selected\n",
            "outer_3: 61 features selected\n",
            "outer_4: 69 features selected\n",
            "outer_5: 64 features selected\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'clf_class__max_depth': 3, 'clf_class__max_features': 1, 'clf_class__n_estimators': 70}      1\n",
              "{'clf_class__max_depth': 4, 'clf_class__max_features': 1, 'clf_class__n_estimators': 55}      1\n",
              "{'clf_class__max_depth': 5, 'clf_class__max_features': 0.2, 'clf_class__n_estimators': 65}    1\n",
              "{'clf_class__max_depth': 5, 'clf_class__max_features': 0.3, 'clf_class__n_estimators': 55}    1\n",
              "{'clf_class__max_depth': 4, 'clf_class__max_features': 0.2, 'clf_class__n_estimators': 65}    1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'clf_class__n_estimators': [50, 55, 60, 65, 70],\n",
        "    'clf_class__max_depth': [3, 4, 5],\n",
        "    'clf_class__max_features': [0.1, 0.2, 0.3, 1],\n",
        "    # 'clf_class__min_samples_split': [7, 11, 15],\n",
        "    # 'clf_class__min_samples_leaf': [1, 2, 3],\n",
        "    # 'clf_class__class_weight': ['balanced', None],\n",
        "    # 'clf_class__criterion': ['gini', 'entropy'],\n",
        "}\n",
        "\n",
        "clf_class = RandomForestClassifier()\n",
        "\n",
        "# NUM_TRIALS = 1 -> could be changed when stronger computer available\n",
        "nested_scores_rf, df_best_params_rf, df_masks_rf = nested_cross_validation(1,clf_class, param_grid, X_clean, y_clean, df_masks_knn)\n",
        "\n",
        "# Save the results to CSV files\n",
        "df_best_params_rf.to_csv(dir_results / \"best_params_rf.csv\", index=False)\n",
        "df_masks_rf.to_csv(dir_results / 'masks_rf.csv', index=False)\n",
        "\n",
        "for i in df_masks_rf:\n",
        "    print(f\"{i}: {df_masks_rf[i].values[0].sum()} features selected\")\n",
        "\n",
        "best_params_rf_ravel= pd.Series(df_best_params_rf.values.ravel())\n",
        "best_params_rf_ravel.value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code for the Support Vector Machine classifier\n",
        "This code is used to perform nested cross-validation for hyperparameter tuning of a Support Vector Machine (SVM) classifier.\n",
        "It includes feature selection using SelectKBest and RFECV, and it evaluates the model's performance using accuracy as the scoring metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outer fold 1\n",
            "Outer fold 2\n",
            "Outer fold 3\n",
            "Outer fold 4\n",
            "Outer fold 5\n",
            "Average performance across 1 trials: 0.7693 ± 0.0000\n",
            "outer_1: 57 features selected\n",
            "outer_2: 72 features selected\n",
            "outer_3: 61 features selected\n",
            "outer_4: 69 features selected\n",
            "outer_5: 64 features selected\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'clf_class__C': 0.1, 'clf_class__gamma': 'scale', 'clf_class__kernel': 'poly'}     2\n",
              "{'clf_class__C': 10, 'clf_class__gamma': 'scale', 'clf_class__kernel': 'linear'}    1\n",
              "{'clf_class__C': 10, 'clf_class__gamma': 'scale', 'clf_class__kernel': 'rbf'}       1\n",
              "{'clf_class__C': 1, 'clf_class__gamma': 'scale', 'clf_class__kernel': 'linear'}     1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Imports\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "param_grid = {\n",
        "    \"clf_class__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
        "    \"clf_class__C\": [0.01, 0.05, 0.1, 1, 5, 10],\n",
        "    \"clf_class__gamma\": [\"scale\", \"auto\"]\n",
        "}\n",
        "\n",
        "clf_class = SVC()\n",
        "\n",
        "# NUM_TRIALS = 1 -> could be changed when stronger computer available\n",
        "nested_scores_svm, df_best_params_svm, df_masks_svm = nested_cross_validation(1,clf_class, param_grid, X_clean, y_clean, df_masks_knn)\n",
        "\n",
        "# Save the results to CSV files\n",
        "df_best_params_svm.to_csv(dir_results / \"best_params_svm.csv\", index=False)\n",
        "df_masks_svm.to_csv(dir_results / 'masks_svm.csv', index=False)\n",
        "\n",
        "for i in df_masks_svm:\n",
        "    print(f\"{i}: {df_masks_svm[i].values[0].sum()} features selected\")\n",
        "\n",
        "best_params_svm_ravel= pd.Series(df_best_params_svm.values.ravel())\n",
        "best_params_svm_ravel.value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Checken voor normaal distributie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "# Keep only numeric features\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# List to store names of normally distributed features\n",
        "normally_distributed = []\n",
        "not_normally_distributed = []\n",
        "# Check normality using Shapiro-Wilk test\n",
        "for col in numeric_df.columns:\n",
        "    stat, p = shapiro(numeric_df[col])\n",
        "    if p >= 0.05:\n",
        "        normally_distributed.append(col)\n",
        "    if p < 0.05:\n",
        "        not_normally_distributed.append(col)\n",
        "\n",
        "# Print the result\n",
        "# print(\"Features with a normal distribution:\")\n",
        "# for feature in normally_distributed:\n",
        "#    print(f\"- {feature}\")\n",
        "# print(\"Features that are not normally distributed:\")\n",
        "# for feature in not_normally_distributed:\n",
        "#    print(f\"- {feature}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Robust Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Apply RobustScaler to the cleaned numeric columns\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Extract numeric columns again (post-cleaning & clipping)\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(numeric_df)\n",
        "\n",
        "# Replace the original numeric columns in df with the scaled version\n",
        "df[numeric_df.columns] = scaled_data\n",
        "\n",
        "# Optional: print confirmation\n",
        "print(\"Applied RobustScaler to numeric features.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PCA Analysis and Visualization\n",
        "\n",
        "This block performs Principal Component Analysis (PCA) to reduce dimensionality and visualize the \n",
        "structure of the dataset in 2D and 3D space. The data has been pre-cleaned and scaled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define your class labels\n",
        "y = df['label']  # \n",
        "\n",
        "# Step 1: Select numeric features\n",
        "X = df.select_dtypes(include='number')\n",
        "\n",
        "# Run full PCA\n",
        "pca = PCA()\n",
        "X_pca_full = pca.fit_transform(X)\n",
        "\n",
        "# Calculate explained variance\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "cumulative_var = np.cumsum(explained_var)\n",
        "\n",
        "# Reduce to 80% variance\n",
        "n_components = np.argmax(cumulative_var >= 0.80) + 1\n",
        "print(f\"Number of components to retain 80% variance: {n_components}\")\n",
        "\n",
        "# Project data to reduced PCA space\n",
        "pca_reduced = PCA(n_components=n_components)\n",
        "X_pca = pca_reduced.fit_transform(X)\n",
        "\n",
        "# 2D PCA plot (PC1 vs PC2), color by class\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('2D PCA Projection (colored by class)')\n",
        "plt.grid(True)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 2], c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 3')\n",
        "plt.title('2D PCA Projection (colored by class)')\n",
        "plt.grid(True)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 1], X_pca[:, 2], c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Principal Component 2')\n",
        "plt.ylabel('Principal Component 3')\n",
        "plt.title('2D PCA Projection (colored by class)')\n",
        "plt.grid(True)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3D PCA plot (PC1 vs PC2 vs PC3), color by class\n",
        "if n_components >= 3:\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    sc = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], \n",
        "                    c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "    ax.set_xlabel('PC1')\n",
        "    ax.set_ylabel('PC2')\n",
        "    ax.set_zlabel('PC3')\n",
        "    ax.set_title('3D PCA Projection (colored by class)')\n",
        "    \n",
        "    # Add legend\n",
        "    legend = ax.legend(*sc.legend_elements(), title=\"Class\", loc=\"upper right\")\n",
        "    ax.add_artist(legend)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough components for 3D plot.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
