{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDn2Sk-VWqE",
        "outputId": "9f162265-cd6c-4b68-fc17-2b6c51b2e291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m852.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ktml (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VfiXrczIa0s"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "92e77de9-3bca-4f23-e8ce-efe538c394a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of samples: 186\n",
            "The number of columns: 494\n"
          ]
        }
      ],
      "source": [
        "# Data loading functions. Uncomment the one you want to use\n",
        "#from worcgist.load_data import load_data\n",
        "#from worclipo.load_data import load_data\n",
        "from worcliver.load_data import load_data\n",
        "#from ecg.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature Cleaning:\n",
        "\n",
        "1. Loading the full dataset from Excel.\n",
        "2. Identifying and removing features with excessive outliers using the IQR method.\n",
        "3. Winsorizing (capping) extreme values in the remaining features to reduce their influence.\n",
        "4. Printing the new dataset dimensions and how many features were removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JZ-2aM8UJx_C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (186, 494)\n",
            "Shape after cleaning: (186, 487)\n",
            "Removed 7 features with more than 17 outliers.\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from an Excel file into a DataFrame\n",
        "df = pd.read_excel('FullDataset.xlsx')\n",
        "print(f\"Original shape: {df.shape}\")  # Shows initial number of rows and columns (samples, features)\n",
        "\n",
        "# Keep only the numeric columns (e.g., float, int) for outlier analysis\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# Dictionary to store number of outliers found in each column\n",
        "outlier_counts = {}\n",
        "\n",
        "# Loop over all numeric features to detect outliers\n",
        "for col in numeric_df.columns:\n",
        "    Q1 = numeric_df[col].quantile(0.25)     # First quartile (25th percentile)\n",
        "    Q3 = numeric_df[col].quantile(0.75)     # Third quartile (75th percentile)\n",
        "    IQR = Q3 - Q1                           # Interquartile range\n",
        "    lower_bound = Q1 - 5 * IQR              # Conservative lower outlier threshold\n",
        "    upper_bound = Q3 + 5 * IQR              # Conservative upper outlier threshold\n",
        "\n",
        "    # Identify outliers and count them\n",
        "    outliers = numeric_df[(numeric_df[col] < lower_bound) | (numeric_df[col] > upper_bound)]\n",
        "    outlier_counts[col] = len(outliers)\n",
        "\n",
        "# Create a DataFrame from the dictionary to make it easy to sort/view\n",
        "outlier_df = pd.DataFrame.from_dict(outlier_counts, orient='index', columns=['outlier_count'])\n",
        "outlier_df = outlier_df.sort_values(by='outlier_count', ascending=False)\n",
        "\n",
        "# Define a threshold: if a feature has more than 17 outliers, we consider it too noisy\n",
        "threshold = 17\n",
        "features_to_drop = outlier_df[outlier_df['outlier_count'] > threshold].index.tolist()\n",
        "\n",
        "# Drop those features from the original DataFrame\n",
        "df = df.drop(columns=features_to_drop)\n",
        "\n",
        "# Identify the features that are still allowed (with 17 or fewer outliers)\n",
        "features_to_winsorize = outlier_df[outlier_df['outlier_count'] <= threshold].index.tolist()\n",
        "\n",
        "# Apply outlier capping to these \"acceptable\" features\n",
        "for feature in features_to_winsorize:\n",
        "    if feature in df.columns:\n",
        "        Q1 = df[feature].quantile(0.25)\n",
        "        Q3 = df[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 5 * IQR\n",
        "        upper_bound = Q3 + 5 * IQR\n",
        "\n",
        "        # Clip values that fall outside the acceptable range\n",
        "        df[feature] = df[feature].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "# Print new shape of the cleaned dataset\n",
        "print(f\"Shape after cleaning: {df.shape}\")\n",
        "print(f\"Removed {len(features_to_drop)} features with more than {threshold} outliers.\")\n",
        "\n",
        "X_clean = df.select_dtypes(include='number').values\n",
        "y_clean = df['label'].values  # Assuming 'label' is the target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function performs nested cross-validation for robust model evaluation and hyperparameter tuning using:\n",
        "- Outer loop: Evaluates generalization performance\n",
        "- Inner loop: Tunes hyperparameters using GridSearchCV\n",
        "- RFECV: Performs recursive feature elimination within each outer fold\n",
        "\n",
        "It returns performance scores, selected hyperparameters, and feature selection masks across multiple repeated trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# difficult nested score to see best params. Perform grid-search with nested cross-validation for hyperparameter tuning\n",
        "\n",
        "def nested_cross_validation(NUM_TRIALS : int, clf_class, param_grid : dict, X : np.ndarray, y : np.ndarray) -> tuple:\n",
        "    \"\"\"\n",
        "    Perform nested cross-validation for hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    - NUM_TRIALS: Number of trials to run\n",
        "    - pipeline: The machine learning pipeline to evaluate\n",
        "    - param_grid: Hyperparameter grid for GridSearchCV\n",
        "    - X: Feature matrix\n",
        "    - y: Target vector\n",
        "\n",
        "    Returns:\n",
        "    - nested_scores: Array of scores from each trial\n",
        "    - df_best_params: List of best parameters for each trial\n",
        "    - df_masks: pandas dataframe of all masks\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    \n",
        "    nested_scores = np.zeros(NUM_TRIALS)      # To store average scores per trial\n",
        "    all_best_params = []                      # To store best hyperparameters per trial\n",
        "    all_feature_masks = []                    # To store feature selection masks per trial\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        (\"scaler\", MinMaxScaler()),  # Add a scaler to the pipeline\n",
        "        (\"clf_class\", clf_class),\n",
        "    ])\n",
        "\n",
        "    for i in range(NUM_TRIALS):\n",
        "        # Outer and inner stratified cross-validation splitters\n",
        "        outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
        "        RFECV_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i+1000)\n",
        "        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i+2000)\n",
        "\n",
        "        # Store the best parameters and scores for each outer fold\n",
        "        best_params = []     # For storing best parameters for each outer fold\n",
        "        outer_scores = []    # AUC scores per outer fold\n",
        "        trial_masks = []     # Feature selection masks per outer fold\n",
        "\n",
        "\n",
        "        for train_idx, test_idx in outer_cv.split(X, y):\n",
        "            print(f\"Outer fold {i+1}\")\n",
        "            # Split data into training and test for this outer fold\n",
        "            X_train, X_test = X[train_idx], X[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "            \n",
        "            # Feature selection via RFECV using the inner CV folds\n",
        "            selector = RFECV(\n",
        "                estimator=SVC(kernel='linear', probability=False),\n",
        "                step=10,\n",
        "                cv=RFECV_cv,\n",
        "                scoring='accuracy',\n",
        "                n_jobs=-1)\n",
        "                  \n",
        "            # Fit the selector on the training data, not the entire dataset         \n",
        "            selector.fit(X_train, y_train)  # Fit RFECV on outer training set\n",
        "            print(f\"RFECV done\")    \n",
        "            trial_masks.append(selector.support_)  # Save which features were kept\n",
        "\n",
        "            # Reduce training and test sets to selected features\n",
        "            X_train = selector.transform(X_train)\n",
        "            X_test = selector.transform(X_test)\n",
        "\n",
        "            # Build a fresh pipeline for each fold\n",
        "            pipeline = Pipeline([\n",
        "                (\"scaler\", MinMaxScaler()),       # Normalize features to [0, 1]\n",
        "                (\"clf_class\", clf_class)              # Insert model\n",
        "            ])\n",
        "                        \n",
        "            # Create a new GridSearchCV for each inner fold\n",
        "            grid_search = GridSearchCV(pipeline, param_grid, cv=inner_cv, scoring=\"roc_auc\", n_jobs=-1)\n",
        "            \n",
        "            # Fit GridSearchCV on the training data\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            \n",
        "            # Get the best model from inner CV\n",
        "            best_model = grid_search.best_estimator_\n",
        "            \n",
        "            # Evaluate the best model on the outer test fold\n",
        "            y_pred = best_model.predict_proba(X_test)[:, 1]\n",
        "            outer_score = roc_auc_score(y_test, y_pred)\n",
        "            \n",
        "            # Append the best parameters and outer score\n",
        "            best_params.append(grid_search.best_params_)\n",
        "            outer_scores.append(outer_score)\n",
        "        \n",
        "        # Store the mean score for this trial\n",
        "        nested_scores[i] = np.mean(outer_scores)\n",
        "        all_best_params.append([i, best_params])\n",
        "        all_feature_masks.append([i, trial_masks])\n",
        "\n",
        "    # Format best params and feature masks into DataFrames\n",
        "    df_best_params = pd.DataFrame([best_params for _, best_params in all_best_params], columns=[\"outer_1\", \"outer_2\", \"outer_3\", \"outer_4\", \"outer_5\"])\n",
        "    df_masks = pd.DataFrame([trial_masks for _, trial_masks in all_feature_masks], columns=[\"outer_1\", \"outer_2\", \"outer_3\", \"outer_4\", \"outer_5\"])\n",
        "\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Average performance across {NUM_TRIALS} trials: {np.mean(nested_scores):.4f} ± {np.std(nested_scores):.4f}\")\n",
        "\n",
        "    return nested_scores, df_best_params, df_masks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "k-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outer fold 1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf_class__n_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m25\u001b[39m],\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m clf_class \u001b[38;5;241m=\u001b[39m KNeighborsClassifier()\n\u001b[1;32m---> 10\u001b[0m nested_scores_knn, df_best_params_knn, df_masks_knn \u001b[38;5;241m=\u001b[39m \u001b[43mnested_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mclf_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m best_params_knn_ravel\u001b[38;5;241m=\u001b[39m df_best_params_knn\u001b[38;5;241m.\u001b[39mSeries(df\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel())\n\u001b[0;32m     13\u001b[0m best_params_knn_ravel\u001b[38;5;241m.\u001b[39mvalue_counts()[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10\u001b[39m]\n",
            "Cell \u001b[1;32mIn[21], line 67\u001b[0m, in \u001b[0;36mnested_cross_validation\u001b[1;34m(NUM_TRIALS, clf_class, param_grid, X, y)\u001b[0m\n\u001b[0;32m     59\u001b[0m selector \u001b[38;5;241m=\u001b[39m RFECV(\n\u001b[0;32m     60\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mSVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     61\u001b[0m     step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     62\u001b[0m     cv\u001b[38;5;241m=\u001b[39mRFECV_cv,\n\u001b[0;32m     63\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     64\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Fit the selector on the training data, not the entire dataset         \u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit RFECV on outer training set\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRFECV done\u001b[39m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m     69\u001b[0m trial_masks\u001b[38;5;241m.\u001b[39mappend(selector\u001b[38;5;241m.\u001b[39msupport_)  \u001b[38;5;66;03m# Save which features were kept\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:873\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups, **params)\u001b[0m\n\u001b[0;32m    870\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    871\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 873\u001b[0m scores_features \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m scores, step_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mscores_features)\n\u001b[0;32m    879\u001b[0m step_n_features_rev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(step_n_features[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'clf_class__n_neighbors': [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25],\n",
        "}\n",
        "\n",
        "clf_class = KNeighborsClassifier()\n",
        "\n",
        "nested_scores_knn, df_best_params_knn, df_masks_knn = nested_cross_validation(1,clf_class, param_grid, X_clean, y_clean)\n",
        "\n",
        "best_params_knn_ravel= df_best_params_knn.Series(df.values.ravel())\n",
        "best_params_knn_ravel.value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf_class__var_smoothing\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m^\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m clf_class \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[1;32m---> 10\u001b[0m nested_scores_naive, df_best_params_naive, df_masks_naive \u001b[38;5;241m=\u001b[39m \u001b[43mnested_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mclf_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m best_params_naive_ravel\u001b[38;5;241m=\u001b[39m df_best_params_naive\u001b[38;5;241m.\u001b[39mSeries(df\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel())\n\u001b[0;32m     13\u001b[0m best_params_naive_ravel\u001b[38;5;241m.\u001b[39mvalue_counts()[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10\u001b[39m]\n",
            "Cell \u001b[1;32mIn[7], line 65\u001b[0m, in \u001b[0;36mnested_cross_validation\u001b[1;34m(NUM_TRIALS, clf_class, param_grid, X, y)\u001b[0m\n\u001b[0;32m     57\u001b[0m selector \u001b[38;5;241m=\u001b[39m RFECV(\n\u001b[0;32m     58\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mSVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     59\u001b[0m     step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     60\u001b[0m     cv\u001b[38;5;241m=\u001b[39minner_cv,\n\u001b[0;32m     61\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     62\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Fit the selector on the training data, not the entire dataset         \u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fit RFECV on outer training set\u001b[39;00m\n\u001b[0;32m     66\u001b[0m trial_masks\u001b[38;5;241m.\u001b[39mappend(selector\u001b[38;5;241m.\u001b[39msupport_)  \u001b[38;5;66;03m# Save which features were kept\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Reduce training and test sets to selected features\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:873\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups, **params)\u001b[0m\n\u001b[0;32m    870\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    871\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 873\u001b[0m scores_features \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m scores, step_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mscores_features)\n\u001b[0;32m    879\u001b[0m step_n_features_rev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(step_n_features[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ljwku\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "param_grid = {\n",
        "    'clf_class__var_smoothing': [1*10^-12, 1*10^-11, 1*10^-10, 1*10^-9, 1*10^-8, 1*10^-7, 1*10^-6, 1*10^-5, 1*10^-4, 1*10^-3, 1*10^-2, 1*10^-1],\n",
        "}\n",
        "\n",
        "clf_class = GaussianNB()\n",
        "\n",
        "nested_scores_naive, df_best_params_naive, df_masks_naive = nested_cross_validation(10,clf_class, param_grid, X_clean, y_clean)\n",
        "\n",
        "best_params_naive_ravel= df_best_params_naive.Series(df.values.ravel())\n",
        "best_params_naive_ravel.value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Forrest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "param_grid = {\n",
        "    'clf_class__var_smoothing': [1*10^-12, 1*10^-11, 1*10^-10, 1*10^-9, 1*10^-8, 1*10^-7, 1*10^-6, 1*10^-5, 1*10^-4, 1*10^-3, 1*10^-2, 1*10^-1],\n",
        "}\n",
        "\n",
        "clf_class = GaussianNB()\n",
        "\n",
        "df_best_params, df_masks = nested_cross_validation(1,clf_class, param_grid, X_clean, y_clean)\n",
        "\n",
        "best_params_naive = pd.Series(df.values.ravel())\n",
        "best_params_naive.value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SVM "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from sklearn import svm\n",
        "\n",
        "param_grid = {\n",
        "    \"svc__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
        "    \"svc__C\": [0.01, 0.05, 0.1, 1, 5, 10],\n",
        "    \"svc__gamma\": [\"scale\", \"auto\"]\n",
        "}\n",
        "\n",
        "clf_class = svm()\n",
        "\n",
        "df_best_params, df_masks = nested_cross_validation(1,clf_class, param_grid, X_clean, y_clean)\n",
        "\n",
        "best_params_naive = pd.Series(df.values.ravel())\n",
        "best_params_naive.value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checken voor normaal distributie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "# Keep only numeric features\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# List to store names of normally distributed features\n",
        "normally_distributed = []\n",
        "not_normally_distributed = []\n",
        "# Check normality using Shapiro-Wilk test\n",
        "for col in numeric_df.columns:\n",
        "    stat, p = shapiro(numeric_df[col])\n",
        "    if p >= 0.05:\n",
        "        normally_distributed.append(col)\n",
        "    if p < 0.05:\n",
        "        not_normally_distributed.append(col)\n",
        "\n",
        "# Print the result\n",
        "# print(\"Features with a normal distribution:\")\n",
        "# for feature in normally_distributed:\n",
        "#    print(f\"- {feature}\")\n",
        "# print(\"Features that are not normally distributed:\")\n",
        "# for feature in not_normally_distributed:\n",
        "#    print(f\"- {feature}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Robust Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Apply RobustScaler to the cleaned numeric columns\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Extract numeric columns again (post-cleaning & clipping)\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(numeric_df)\n",
        "\n",
        "# Replace the original numeric columns in df with the scaled version\n",
        "df[numeric_df.columns] = scaled_data\n",
        "\n",
        "# Optional: print confirmation\n",
        "print(\"Applied RobustScaler to numeric features.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PCA Analysis and Visualization\n",
        "\n",
        "This block performs Principal Component Analysis (PCA) to reduce dimensionality and visualize the \n",
        "structure of the dataset in 2D and 3D space. The data has been pre-cleaned and scaled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define your class labels\n",
        "y = df['label']  # \n",
        "\n",
        "# Step 1: Select numeric features\n",
        "X = df.select_dtypes(include='number')\n",
        "\n",
        "# Run full PCA\n",
        "pca = PCA()\n",
        "X_pca_full = pca.fit_transform(X)\n",
        "\n",
        "# Calculate explained variance\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "cumulative_var = np.cumsum(explained_var)\n",
        "\n",
        "# Reduce to 80% variance\n",
        "n_components = np.argmax(cumulative_var >= 0.80) + 1\n",
        "print(f\"Number of components to retain 80% variance: {n_components}\")\n",
        "\n",
        "# Project data to reduced PCA space\n",
        "pca_reduced = PCA(n_components=n_components)\n",
        "X_pca = pca_reduced.fit_transform(X)\n",
        "\n",
        "# 2D PCA plot (PC1 vs PC2), color by class\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('2D PCA Projection (colored by class)')\n",
        "plt.grid(True)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 2], c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 3')\n",
        "plt.title('2D PCA Projection (colored by class)')\n",
        "plt.grid(True)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 1], X_pca[:, 2], c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Principal Component 2')\n",
        "plt.ylabel('Principal Component 3')\n",
        "plt.title('2D PCA Projection (colored by class)')\n",
        "plt.grid(True)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3D PCA plot (PC1 vs PC2 vs PC3), color by class\n",
        "if n_components >= 3:\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    sc = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], \n",
        "                    c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "    ax.set_xlabel('PC1')\n",
        "    ax.set_ylabel('PC2')\n",
        "    ax.set_zlabel('PC3')\n",
        "    ax.set_title('3D PCA Projection (colored by class)')\n",
        "    \n",
        "    # Add legend\n",
        "    legend = ax.legend(*sc.legend_elements(), title=\"Class\", loc=\"upper right\")\n",
        "    ax.add_artist(legend)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough components for 3D plot.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
