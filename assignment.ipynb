{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDn2Sk-VWqE",
        "outputId": "9f162265-cd6c-4b68-fc17-2b6c51b2e291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m852.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ktml (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VfiXrczIa0s"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "92e77de9-3bca-4f23-e8ce-efe538c394a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of samples: 186\n",
            "The number of columns: 494\n"
          ]
        }
      ],
      "source": [
        "# Data loading functions. Uncomment the one you want to use\n",
        "#from worcgist.load_data import load_data\n",
        "#from worclipo.load_data import load_data\n",
        "from worcliver.load_data import load_data\n",
        "#from ecg.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature Cleaning:\n",
        "\n",
        "1. Loading the full dataset from Excel.\n",
        "2. Identifying and removing features with excessive outliers using the IQR method.\n",
        "3. Winsorizing (capping) extreme values in the remaining features to reduce their influence.\n",
        "4. Printing the new dataset dimensions and how many features were removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ-2aM8UJx_C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (186, 494)\n",
            "Shape after cleaning: (186, 487)\n",
            "Removed 7 features with more than 17 outliers.\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset from an Excel file into a DataFrame\n",
        "df = pd.read_excel('FullDataset.xlsx')\n",
        "print(f\"Original shape: {df.shape}\")  # Shows initial number of rows and columns (samples, features)\n",
        "\n",
        "# Keep only the numeric columns (e.g., float, int) for outlier analysis\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# Dictionary to store number of outliers found in each column\n",
        "outlier_counts = {}\n",
        "\n",
        "# Loop over all numeric features to detect outliers\n",
        "for col in numeric_df.columns:\n",
        "    Q1 = numeric_df[col].quantile(0.25)     # First quartile (25th percentile)\n",
        "    Q3 = numeric_df[col].quantile(0.75)     # Third quartile (75th percentile)\n",
        "    IQR = Q3 - Q1                           # Interquartile range\n",
        "    lower_bound = Q1 - 5 * IQR              # Conservative lower outlier threshold\n",
        "    upper_bound = Q3 + 5 * IQR              # Conservative upper outlier threshold\n",
        "\n",
        "    # Identify outliers and count them\n",
        "    outliers = numeric_df[(numeric_df[col] < lower_bound) | (numeric_df[col] > upper_bound)]\n",
        "    outlier_counts[col] = len(outliers)\n",
        "\n",
        "# Create a DataFrame from the dictionary to make it easy to sort/view\n",
        "outlier_df = pd.DataFrame.from_dict(outlier_counts, orient='index', columns=['outlier_count'])\n",
        "outlier_df = outlier_df.sort_values(by='outlier_count', ascending=False)\n",
        "\n",
        "# Define a threshold: if a feature has more than 17 outliers, we consider it too noisy\n",
        "threshold = 17\n",
        "features_to_drop = outlier_df[outlier_df['outlier_count'] > threshold].index.tolist()\n",
        "\n",
        "# Drop those features from the original DataFrame\n",
        "df = df.drop(columns=features_to_drop)\n",
        "\n",
        "# Identify the features that are still allowed (with 17 or fewer outliers)\n",
        "features_to_winsorize = outlier_df[outlier_df['outlier_count'] <= threshold].index.tolist()\n",
        "\n",
        "# Apply outlier capping to these \"acceptable\" features\n",
        "for feature in features_to_winsorize:\n",
        "    if feature in df.columns:\n",
        "        Q1 = df[feature].quantile(0.25)\n",
        "        Q3 = df[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 5 * IQR\n",
        "        upper_bound = Q3 + 5 * IQR\n",
        "\n",
        "        # Clip values that fall outside the acceptable range\n",
        "        df[feature] = df[feature].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "# Print new shape of the cleaned dataset\n",
        "print(f\"Shape after cleaning: {df.shape}\")\n",
        "print(f\"Removed {len(features_to_drop)} features with more than {threshold} outliers.\")\n",
        "\n",
        "X_clean = df.select_dtypes(include='number').values\n",
        "y_clean = df['label'].values  # Assuming 'label' is the target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function performs nested cross-validation for robust model evaluation and hyperparameter tuning using:\n",
        "- Outer loop: Evaluates generalization performance\n",
        "- Inner loop: Tunes hyperparameters using GridSearchCV\n",
        "- RFECV: Performs recursive feature elimination within each outer fold\n",
        "\n",
        "It returns performance scores, selected hyperparameters, and feature selection masks across multiple repeated trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# difficult nested score to see best params. Perform grid-search with nested cross-validation for hyperparameter tuning\n",
        "\n",
        "def nested_cross_validation(NUM_TRIALS : int, clf_class, param_grid : dict, X : np.ndarray, y : np.ndarray) -> tuple:\n",
        "    \"\"\"\n",
        "    Perform nested cross-validation for hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    - NUM_TRIALS: Number of trials to run\n",
        "    - pipeline: The machine learning pipeline to evaluate\n",
        "    - param_grid: Hyperparameter grid for GridSearchCV\n",
        "    - X: Feature matrix\n",
        "    - y: Target vector\n",
        "\n",
        "    Returns:\n",
        "    - nested_scores: Array of scores from each trial\n",
        "    - df_best_params: List of best parameters for each trial\n",
        "    - df_masks: pandas dataframe of all masks\n",
        "    \"\"\"\n",
        "\n",
        "    NUM_TRIALS = NUM_TRIALS\n",
        "    \n",
        "    nested_scores = np.zeros(NUM_TRIALS)      # To store average scores per trial\n",
        "    all_best_params = []                      # To store best hyperparameters per trial\n",
        "    all_feature_masks = []                    # To store feature selection masks per trial\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        (\"scaler\", MinMaxScaler()),  # Add a scaler to the pipeline\n",
        "        (\"clf_class\", clf_class),\n",
        "    ])\n",
        "\n",
        "    for i in range(NUM_TRIALS):\n",
        "        # Outer and inner stratified cross-validation splitters\n",
        "        outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
        "        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=i+1000)\n",
        "\n",
        "        # Store the best parameters and scores for each outer fold\n",
        "        best_params = []     # For storing best parameters for each outer fold\n",
        "        outer_scores = []    # AUC scores per outer fold\n",
        "        trial_masks = []     # Feature selection masks per outer fold\n",
        "\n",
        "\n",
        "        for train_idx, test_idx in outer_cv.split(X, y):\n",
        "            # Split data into training and test for this outer fold\n",
        "            X_train, X_test = X[train_idx], X[test_idx]\n",
        "            y_train, y_test = y[train_idx], y[test_idx]\n",
        "            \n",
        "            # Feature selection via RFECV using the inner CV folds\n",
        "            selector = RFECV(\n",
        "                estimator=SVC(kernel='linear', probability=True),\n",
        "                step=1,\n",
        "                cv=inner_cv,\n",
        "                scoring='roc_auc',\n",
        "                n_jobs=-1)\n",
        "                        \n",
        "            # Fit the selector on the training data, not the entire dataset         \n",
        "            selector.fit(X_train, y_train)  # Fit RFECV on outer training set\n",
        "            trial_masks.append(selector.support_)  # Save which features were kept\n",
        "\n",
        "            # Reduce training and test sets to selected features\n",
        "            X_train = selector.transform(X_train)\n",
        "            X_test = selector.transform(X_test)\n",
        "\n",
        "            # Build a fresh pipeline for each fold\n",
        "            pipeline = Pipeline([\n",
        "                (\"scaler\", MinMaxScaler()),       # Normalize features to [0, 1]\n",
        "                (\"clf_class\", clf_class)              # Insert model\n",
        "            ])\n",
        "                        \n",
        "            # Create a new GridSearchCV for each inner fold\n",
        "            grid_search = GridSearchCV(pipeline, param_grid, cv=inner_cv, scoring=\"roc_auc\", n_jobs=-1)\n",
        "            \n",
        "            # Fit GridSearchCV on the training data\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            \n",
        "            # Get the best model from inner CV\n",
        "            best_model = grid_search.best_estimator_\n",
        "            \n",
        "            # Evaluate the best model on the outer test fold\n",
        "            y_pred = best_model.predict_proba(X_test)[:, 1]\n",
        "            outer_score = roc_auc_score(y_test, y_pred)\n",
        "            \n",
        "            # Append the best parameters and outer score\n",
        "            best_params.append(grid_search.best_params_)\n",
        "            outer_scores.append(outer_score)\n",
        "        \n",
        "        # Store the mean score for this trial\n",
        "        nested_scores[i] = np.mean(outer_scores)\n",
        "        all_best_params.append([i, best_params])\n",
        "        all_feature_masks.append([i, trial_masks])\n",
        "\n",
        "    # Format best params and feature masks into DataFrames\n",
        "    df_best_params = pd.DataFrame([best_params for _, best_params in all_best_params], columns=[\"outer_1\", \"outer_2\", \"outer_3\", \"outer_4\", \"outer_5\"])\n",
        "    df_masks = pd.DataFrame([trial_masks for _, trial_masks in all_feature_masks], columns=[\"outer_1\", \"outer_2\", \"outer_3\", \"outer_4\", \"outer_5\"])\n",
        "\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Average performance across {NUM_TRIALS} trials: {np.mean(nested_scores):.4f} ± {np.std(nested_scores):.4f}\")\n",
        "\n",
        "    return nested_scores, df_best_params, df_masks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "k-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "param_grid = {\n",
        "    'clf_class__n_neighbors': [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25],\n",
        "}\n",
        "\n",
        "clf_class = KNeighborsClassifier()\n",
        "\n",
        "df_best_params, df_masks = nested_cross_validation(10,clf_class, param_grid, X_clean.values, y.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = pd.Series(df.values.ravel())\n",
        "best_params.value_counts()[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checken voor normaal distributie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "# Keep only numeric features\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# List to store names of normally distributed features\n",
        "normally_distributed = []\n",
        "not_normally_distributed = []\n",
        "# Check normality using Shapiro-Wilk test\n",
        "for col in numeric_df.columns:\n",
        "    stat, p = shapiro(numeric_df[col])\n",
        "    if p >= 0.05:\n",
        "        normally_distributed.append(col)\n",
        "    if p < 0.05:\n",
        "        not_normally_distributed.append(col)\n",
        "\n",
        "# Print the result\n",
        "# print(\"Features with a normal distribution:\")\n",
        "# for feature in normally_distributed:\n",
        "#    print(f\"- {feature}\")\n",
        "# print(\"Features that are not normally distributed:\")\n",
        "# for feature in not_normally_distributed:\n",
        "#    print(f\"- {feature}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Robust Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Apply RobustScaler to the cleaned numeric columns\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Extract numeric columns again (post-cleaning & clipping)\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(numeric_df)\n",
        "\n",
        "# Replace the original numeric columns in df with the scaled version\n",
        "df[numeric_df.columns] = scaled_data\n",
        "\n",
        "# Optional: print confirmation\n",
        "print(\"Applied RobustScaler to numeric features.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PCA Analysis and Visualization\n",
        "\n",
        "This block performs Principal Component Analysis (PCA) to reduce dimensionality and visualize the \n",
        "structure of the dataset in 2D and 3D space. The data has been pre-cleaned and scaled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define your class labels\n",
        "y = df['label']  # \n",
        "\n",
        "# Step 1: Select numeric features\n",
        "X = df.select_dtypes(include='number')\n",
        "\n",
        "# Run full PCA\n",
        "pca = PCA()\n",
        "X_pca_full = pca.fit_transform(X)\n",
        "\n",
        "# Calculate explained variance\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "cumulative_var = np.cumsum(explained_var)\n",
        "\n",
        "# Reduce to 80% variance\n",
        "n_components = np.argmax(cumulative_var >= 0.80) + 1\n",
        "print(f\"Number of components to retain 80% variance: {n_components}\")\n",
        "\n",
        "# Project data to reduced PCA space\n",
        "pca_reduced = PCA(n_components=n_components)\n",
        "X_pca = pca_reduced.fit_transform(X)\n",
        "\n",
        "# 2D PCA plot (PC1 vs PC2), color by class\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('2D PCA Projection (colored by class)')\n",
        "plt.grid(True)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 2], c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 3')\n",
        "plt.title('2D PCA Projection (colored by class)')\n",
        "plt.grid(True)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_pca[:, 1], X_pca[:, 2], c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "plt.xlabel('Principal Component 2')\n",
        "plt.ylabel('Principal Component 3')\n",
        "plt.title('2D PCA Projection (colored by class)')\n",
        "plt.grid(True)\n",
        "plt.legend(*scatter.legend_elements(), title=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3D PCA plot (PC1 vs PC2 vs PC3), color by class\n",
        "if n_components >= 3:\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    sc = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], \n",
        "                    c=y, cmap='Set1', edgecolor='k', alpha=0.7)\n",
        "    ax.set_xlabel('PC1')\n",
        "    ax.set_ylabel('PC2')\n",
        "    ax.set_zlabel('PC3')\n",
        "    ax.set_title('3D PCA Projection (colored by class)')\n",
        "    \n",
        "    # Add legend\n",
        "    legend = ax.legend(*sc.legend_elements(), title=\"Class\", loc=\"upper right\")\n",
        "    ax.add_artist(legend)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough components for 3D plot.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
